---
title: Predicting Residential Home Sales Prices Using Regression Analysis
author:
- Rumil Legaspi, Rumil.legaspi@gmail.com
- Nancy Huerta, Nhuerta20@apu.edu
date: "11 April 2021"
output:
  word_document: default
  html_notebook:
    df_print: paged
  pdf_document:
    toc: yes
    toc_depth: 2
    latex_engine: xelatex
  theme: lumen
  html_document:
    toc: yes
    toc_depth: '2'
    df_print: paged
---

# **Purpose**

We are conducting a **multiple linear regression** from the Real Estate Sales (APPENC07) dataset to analyze the relationship of the given features, ***bedrooms***, ***bathrooms***, and ***garage size***, with the outcome variable, ***house sales price*** in a midwestern city.

\newpage

# **Our Data**

## Background on Dataset & Variables

Our dataset is comprised of *522 total transactions* from home sales during the year 2002.

+---------------------------------------+----------------------------------+----------------------------------+------------------------------------+
| Response Variable (Y)                 | Explanatory Variable 1 ($X_{1}$) | Explanatory Variable 2 ($X_{2}$) | Explanatory Variable 3 ($X_{3}$)   |
+=======================================+==================================+==================================+====================================+
| "house_price"                         | "beds"                           | "baths"                          | "garage_size"                      |
+---------------------------------------+----------------------------------+----------------------------------+------------------------------------+
| sales price of residence (in dollars) | Number of bedrooms               | Number of bathrooms              | Number of cars the garage can hold |
+---------------------------------------+----------------------------------+----------------------------------+------------------------------------+

```{r, collapse = TRUE, warning = FALSE, message = FALSE}

#Setting up our work environment
setwd("C:/Users/RUMIL/Desktop/APU/STAT 511 - Millie Mao (Applied Regression Analysis)/Project 2")
library(nortest)
library(olsrr)
library(car)
library(lmtest)
library(MASS)
library(tidyverse)
library(ggcorrplot)
#Loading in the text data
raw_data = read.table(file = "APPENC07.txt", header = FALSE, sep = "")

#Converting into tibble data frame for easier data analysis
house_data <- as_tibble(raw_data)
```

```{r}
#Defining and renaming our Explanatory(X) and Response(Y) variables
house_data <- house_data %>% select(house_price = V2,
                                    beds = V4,
                                    baths = V5,
                                    garage_size = V7)

#Setting explanatory and response variables
house_price <-  house_data %>% select(house_price) #Y
beds <- house_data %>% select(beds) #X1
baths <- house_data %>% select(baths) #X2
garage_size <- house_data %>% select(garage_size) #X3

```

\newpage

# Part 1 - Model Estimation and Interpretation

## 1a. Fitting a regression model estimating sales price using beds, baths, and garage size as predicting variables

```{r}
#Using the lm function to fit a multiple regression model
house_lm <- lm(house_price ~ beds + baths + garage_size, data = house_data)

#Regression summary
summary(house_lm)
```

## 1b. Interpretation of Coefficients

### Intercept & Partial Slopes

From summarizing our multiple linear regression model we can see:

| **Intercept** | Bedrooms    | Bathrooms   | Garage Size |
|---------------|-------------|-------------|-------------|
| $\beta_{0}$   | $\beta_{1}$ | $\beta_{2}$ | $\beta_{3}$ |
| -45886.3      | 935.4       | 67818.9     | 67332.3     |

and the estimated regression equation to be:

$\hat{Y}= -45886.3 + 935.4X + 67818.9X + 67332.3X$

The partial slopes in our summary indicate that when any one of the partial slopes **Increase by 1 unit** and other explanatory variables held constant and unchanged we can expect:

-   While holding our other explanatory variables Bathrooms and Garage Size constant and unchanged, when **Bedrooms** increase by 1 unit, we can expect our **house sales price** to increase by **roughly \$935.4**.

-   While holding our other explanatory variables Bedrooms and Garage Size constant and unchanged, when **Bathrooms** increases by 1 unit, we can expect our **house sales price** to increase by **roughly \$67,818.9.**

-   While holding our other explanatory variables Bedrooms and Bathrooms constant and unchanged, when **Garage size** increases by 1 unit, we can expect our **house sales price** to increase by **roughly \$67,332.3**.

## 1c. Interpretation of Adjusted R-Squared = 0.54

A adjusted R squared value, similar to the R square value, tells us how much of the variability in our model is explained by our predictor variables, while also penalizing redundant or otherwise useless predictor variables helping us to resist urges of adding too many variables into our model.

In this case our adjusted \$R\^2\$ of 0.54 tells us that about 54% of the variation in our response variable is explained by our 3 explanatory variables.

# Part 2 - Prediction

## 2a. Predicting the house sales price for a house with 3 bedrooms, 3 bathrooms, and a 2-car garage

```{r}
#We create an artifical observation where a given house has
#3 Bedrooms, 3 Bathrooms, and a 2 car garage
new_house_data <- data.frame(beds = 3, baths = 3, garage_size = 2)
```

## 2b. Calculating the 95% confidence interval

```{r}
#confidence interval
ci_house <- predict(house_lm, new_house_data, interval = "confidence", level = 0.95)
ci_house
```

### Interpretation

This 95% confidence interval, when Bedrooms = 3, Bathrooms = 3, and Garage Size = 2, is from **74.84094 to 79.70906.**

When Bedrooms = 3, Bathrooms = 3, and Garage Size = 2, with 95% confidence we can expect our confidence interval to capture the average of house sales prices (response variable).

## 2c. Calculating the 95% prediction interval

```{r}
#prediction interval
pi_house <- predict(house_lm, new_house_data, interval = "prediction", level = 0.95)
pi_house
```

### Interpretation

From the results we can predict with 95% confidence that when their are 3 bedrooms, 3 bathrooms, and a garage that can hold 2 cars, the predicted house sales price will fall somewhere between **111,422 to 478,660 dollars.**

# Part 3 - Hypothesis Testing

## 3a. Checking the significance for each individual partial slope (independent variable)

### Using a significance level of $\alpha = 0.05$

**Null Hypothesis**: ${H_0}$: $\beta_{j} = 0$ (slopes are showing no change), $X_{j}$ **is not** linearly associated with Y, therefore the partial slope **is not significant.**

**Alternative Hypothesis**: ${H_1}$: $\beta_{j} \neq 0$ (slopes are showing change), $X_{j}$ **is** linearly associated with Y, therefore the partial slope **is significant.**

|                                |                                 |                                 |
|--------------------------------|---------------------------------|---------------------------------|
| Bedrooms ($X_{1}$)             | Bathrooms ($X_{2}$)             | Garage Size ($X_{3}$)           |
| 0.8507 **\>** $\alpha = 0.05%$ | \<2e-16 **\<** $\alpha = 0.05%$ | \<2e-16 **\<** $\alpha = 0.05%$ |
| Fail to reject ${H_0}$         | Reject ${H_0}$                  | Reject ${H_0}$                  |
| Not Significant                | Significant                     | Significant                     |
|                                |                                 |                                 |

: Table Representation of Hypothesis Testing

**Bedroom variable:**

The p-value of Bedroom is 0.8507 and is greater than our $\alpha$ (accepted error) of 0.05, so we **fail to reject** our NULL hypothesis and must conclude with our NULL hypothesis. Stating that our partial slope, **Bedrooms**, does not show significance in our model.

**Bathroom & Garage Size variables:**

On the other hand because the p-value of Bathroom and Garage size are both \<2e-16 and are incredibly smaller than our $\alpha$ (accepted error) of 0.05, so we **reject** our NULL hypothesis and conclude with our alternative hypothesis. Our alternative hypothesis states that our partial slopes, **Bathroom and Garage Size**, shows significance in our model.

## 3b. Conducting an F-test to check overall model significance

### Using a significance level of $\alpha = 0.05$

**Null Hypothesis**: ${H_0}$: $\beta_{1}=\beta_{2} = 0$ (**No** partial slopes are significant). Shows no change, therefore **does not** show overall model significance.

**Alternative Hypothesis**: ${H_1}$: $\beta_{1}=\beta_{2} \neq 0$ (**At least one** partial slope is significant). Shows change, therefore **showing** overall model significance

```{r}
#We can use the qt() to find our critical value and compare with our t-value (test statistic)
# We use 0.95 Because of our 95% confidence interval and 518 for our degrees of freedom
qt(0.975, 518)

#Checking for our f-value 
anova(house_lm)
```

+------------------------------+--------------------+---------------------+-----------------------+
| Test Statistic Type & Result | Bedrooms ($X_{1}$) | Bathrooms ($X_{2}$) | Garage Size ($X_{3}$) |
+==============================+====================+=====================+=======================+
| F-value                      | 194.515 \> 1.96    | 338.057 \> 1.96     | 88.032 \> 1.96        |
+------------------------------+--------------------+---------------------+-----------------------+
| P-value                      | 2.2e-16 \< 0.05    | 2.2e-16 \< 0.05     | 2.2e-16 \< 0.05       |
+------------------------------+--------------------+---------------------+-----------------------+
| Result                       | Significant        | Significant         | Significant           |
+------------------------------+--------------------+---------------------+-----------------------+

: Table Representation of F-test Hypothesis Testing

Our p-value of \< 2.2e-16 being less than our alpha and our F values being larger then our critical value tells us we can **reject** our NULL hypothesis and conclude with our alternative hypothesis, that **at least one** of our predictor variables **shows** overall model significance.

## 3c. Conducting Partial F tests

### Which variable is actually contributing?

Conducting partial F tests is important to see if the number of bathrooms (X2) and garage size(X3) are **jointly significant.**

### Using a significance level of 0.05

**Null Hypothesis**: ${H_0}$: ***There is no*** change when adding certain predictors to the significance of our model

**Alternative Hypothesis**: ${H_1}$: ***There is*** change when adding certain predictors towards the significance of our model

```{r}
#full model
house_lm

#reduced model without bathrooms and garage size
bed_lm <- lm(house_price ~ beds, data = house_data)
```

We now ***compare*** our reduced model with our complete model

```{r}
anova(bed_lm, house_lm)

```

Since the p-value is **2.2e-16** is less than our significance level of 0.05 we see that bathroom and garage size are both jointly significant and therefore we can reject the null hypothesis, indicating there is significance in keeping both bathroom and garage size in our model.

In effect, we are concluding that bathroom and garage size are predictors that do contribute information in the prediction of house sales price and therefore should be retained in the model.

# Part 4 - Multicolinearity

### Why bother with multicolinearity?

Having multicolinearity is problematic because by having multiple correlated predictor variables, it becomes harder for our model to attribute significance to our predictor variables. It creates redundant and duplicate information, thereby negatively affecting the results of our regression model.

## 4a. Creating scatterplots and correlation matrices

```{r}
#Plotting a scatterplot matrix **(why does it look symmterical?)
scat_matrix <- c(beds, baths, garage_size) %>% 
  data.frame() %>%
  plot()

scat_matrix

#Correlation Matrix
corr_matrix <- c(beds, baths, garage_size) %>% 
  data.frame() %>% 
  cor()

corr_matrix

ggcorr_matrix <- ggcorrplot(corr_matrix, hc.order = TRUE, type = "lower", lab = TRUE,
   outline.col = "white",
   ggtheme = ggplot2::theme_gray,
   colors = c("#6D9EC1", "white", "#E46726"))

#Printing both matrices
scat_matrix
ggcorr_matrix
summary(house_lm)
corr_matrix
```

## 4b. Removing Two Strongly Correlated Variables

A way to combat this is by removing a highly correlated predictor. From the correlation matrix and by looking at our correlation coefficient, we can see moderately positive relationship between bedrooms and bathrooms which might be worth further investigating.

We know there is some kind of multicolinearity issue with bed and baths, I am more interested in beds than baths. So I can remove baths from our model thereby correcting our multicolinearity issue.

```{r}
#summary of original predictor
summary(house_lm)
```

We notice bedrooms is not a significant variable from looking at the p-value, when in fact, the reality is it **should** be significant. Knowing this, we can check to see how well our model performs when removing bathroom since there is a multicolinearity issue.

```{r}
#removing beds**
nobeds_lm <- lm(house_price ~ baths + garage_size, data = house_data)
summary(nobeds_lm)
```

```{r}
#removing baths
nobaths_lm <- lm(house_price ~ garage_size + beds, data = house_data)
summary(nobaths_lm)
```

In both cases we can see that by either removing bedroom or bathroom in our model, the predictors still remain significant but more importantly we can now see that bedrooms is in fact a significant predictor when removing the bathroom variable in our model concluding that we have addressed our issue of multicolinearity.
