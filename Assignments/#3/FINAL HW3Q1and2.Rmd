---
title: 'STAT 511: HW #3 Q:1 & 2'
author: "Rumil Legaspi"
date: "15 February 2021"
output:
  pdf_document: 
    latex_engine: xelatex
  html_notebook: default
  html_document:
    df_print: paged
  word_document: default
---

## __1. Refer to the GPA problem in HW#1__

#### __Workspace Setup__

```{r}
setwd("C:/Users/RUMIL/Desktop/APU/STAT 511 - Millie Mao (Applied Regression Analysis)/week 4/Week 4")

gpa_data = read.table(file = "GPA.txt", header = FALSE, sep = "")

#Adding headers
names(gpa_data) <- c("GPA", "ACT")

#Defining dependent and independent vars
ACT = gpa_data$ACT #X
GPA = gpa_data$GPA #Y

gpa_lm = lm(GPA ~ ACT, data = gpa_data)
summary(gpa_lm)

```



### __(a). Setting up the ANOVA table__ 

```{r}
anova(gpa_lm)
```


### __(b). What does MSR measure in your ANOVA table? What does MSE measure? Under what condition do MSR and MSE estimate the same quantity?__

To begin with, the MSR (Mean Squared of Regression) comes from the sum of squares (of our X values) divided by the degrees of freedom from our regression model. In this case MSR measures our regression model's variability (separate from the variability of our error(MSE)).

MSE is the mean squared error and measures how close our regression line is to the data points. It measures the distances from the data points to our regression line. Those distances are the errors and the MSE is squaring those errors. __Ultimately__, it measures the variability/spread of those errors.


Both the MSR and MSE estimate the same quantity when the slope  $\beta_{1}$ is zero or not.


### __(c). At $\alpha$ = 0.05, conduct an F-test of whether or not $\beta_{1}$ = 0. State the null and alternative hypotheses, decision rule, and conclusion.__

<div align="center">

Null hypothesis: H0: $\beta_{1}$ = 0 (slope is horizontal/ no relationship)

Alternative hypothesis: H1: $\beta_{1}$ /= 0 (slope exists/ relationship exists)
      
</div>

Given our F-value is 9.2402 

__note: null rejection rules__ 

_1. If the f statistic is larger than the critical value_ 

_2. Or the p value is less than alpha then it is significant and we reject the null_

```{r}
#Using qt() to find our critical value we get
#Since alternative is not equal we're looking at both tails
# .025 on both sides
qt(0.975, 118)

```

The F-value is greater than our critical value (1.980272) __AND__ our p-value (0.002917) is less than our $\alpha$ = 0.05 we reject our null and therefore a non-zero slope exists, as well as a significant relationship between ACT(X) and GPA(Y) scores.


### __(d).  Obtain the R-squared from your regression. Interpret this number__


<div align="center">

- R squared = 1 - unexplained variation/total variation

- R squared = variability in Y explained by X/total variability

- R squared = SSR(sum of squares of regression)/SST(sum of squares of total variation(SSR+SSE))

- R squared = 1 - SSE(sum of squares of error)/SST(sum of squares of total variation(SSR+SSE))

  
</div>

```{r}
#Using R functions to find R squared
summary(gpa_lm)$r.squared

#Reading from ANOVA table we can calculate manually
anova(gpa_lm)

#SSR(sum of squares of regression)
SSR_gpa <- 3.58

#SSE(sum of squares of errors)
SSE_gpa <- 45.818

#SST(sum of squares of total variation)

SST_gpa <- SSR_gpa + SSE_gpa

#R squared
rsquared_gpa <- SSR_gpa/ SST_gpa
rsquared_gpa
```


The R squared we found [1] 0.07247257 indicates that our ACT scores (input variable) explains close 7% of the variability in our dependent variable GPA. __The relationship between our model and the dependent variable GPA is very weak.__


--------------------------------------------

## __2. Refer to the Muscle Mass problem in HW#1.__

#### __Workspace Setup__ 

```{r}
muscle_data = read.table(file = "Muscle.txt", header = FALSE, sep = "")

#Adding headers
names(muscle_data) <- c("Muscle", "Age")

#Defining dependent and independent vars
Age = muscle_data$Age #X
Muscle = muscle_data$Muscle #Y

#creating our linear model
muscle_lm = lm(Muscle ~ Age, data = muscle_data)
summary(muscle_lm)
```


### __(a). Setting up ANOVA Table__

```{r}
anova(muscle_lm)
```

### __(b). At $\alpha$ = 0.05, conduct an F-test of whether or not $\beta_{1}$ = 0. State the null and alternative hypotheses, decision rule, and conclusion.__


<div align="center">

Null hypothesis: H0: $\beta_{1}$ = 0 (slope is horizontal/ no relationship between X and Y)

Alternative hypothesis: H1: $\beta_{1}$ /= 0 (slope is non zero and X is linearly related to Y)
      
</div>

```{r}
#Using qt() to find our critical value we get
#Since alternative is not equal we're looking at both tails
# .025 on both sides
qt(0.975, 58)
```
 Our F-statistic = 174.06

The F-value (174.06) is greater than our critical value (2.001717) __AND__ our p-value (2.2e-16) is less than our $\alpha$ = 0.05, we reject our null and therefore a non-zero slope exists, as well as a significant relationship between AGE(X) and MUSCLE(Y).

### __(c). What proportion of the total variation in Muscle Mass remains “unexplained” in the regression with Age? Is this proportion relatively small or large?__

The unexplained variation is the error component of the regression equation. It is the mean squared error (MSE) which is the sum of squares divided by the degrees of freedom.

```{r}
#explained variation, From summary
rsquared.muscle <- 0.7501
#Unexplained variation
1 - rsquared.muscle
```
~25% is unexplained, this number is small and therefore our model is strong since our explained variability is ~75%.

The ratio  11627.5 (MSR)/ 66.8 (MSE) shows a ratio that is fairly relatively large. We can definitely see that our model is contributing far more to the variance than the error is. Which is also true when we compare our F statistic (174.06) with the critical value (2.001717) and see that our model is a significant in predicting the variance between age and muscle mass. 


### __(d).  Obtain the R-squared from your regression. Interpret this number__


```{r}
#Using R functions to find R squared
summary(muscle_lm)$r.squared

#Reading from ANOVA table we can calculate manually
anova(muscle_lm)

#SSR(sum of squares of regression)
SSR_muscle <- 11627.5

#SSE(sum of squares of errors)
SSE_muscle <- 3874.4

#SST(sum of squares of total variation)

SST_muscle <- SSR_muscle + SSE_muscle

#R squared
rsquared_muscle <- SSR_muscle / SST_muscle
rsquared_muscle
```


The R squared we found [1] 0.7500693 indicates that Age (input variable) explains helps explain close to 75% of the variability in our dependent variable Muscle. In other words, __the relationship between our model and the dependent variable Muscle is strong.__


----------------------------------------------
