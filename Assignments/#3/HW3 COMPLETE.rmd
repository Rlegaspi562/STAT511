---
title: 'STAT 511: HW #3 Q:1 & 2'
author: "Rumil Legaspi"
date: "21 February 2021"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: 
    toc: yes
    toc_depth: 2
    latex_engine: xelatex
  word_document: default
---

# __1. Refer to the GPA problem in HW#1__

### __Workspace Setup__

    ```{r}
setwd("C:/Users/RUMIL/Desktop/APU/STAT 511 - Millie Mao (Applied Regression Analysis)/week 4/Week 4")

gpa_data = read.table(file = "GPA.txt", header = FALSE, sep = "")

#Adding headers
names(gpa_data) <- c("GPA", "ACT")

#Defining dependent and independent vars
ACT = gpa_data$ACT #X
GPA = gpa_data$GPA #Y

gpa_lm = lm(GPA ~ ACT, data = gpa_data)
summary(gpa_lm)

```



## __(a). Setting up the ANOVA table__

```{r}
anova(gpa_lm)
```


## __(b). What does MSR measure in your ANOVA table? What does MSE measure? Under what condition do MSR and MSE estimate the same quantity?__

To begin with, the MSR (Mean Squared of Regression) comes from the sum of squares (of our X values) divided by the degrees of freedom from our regression model. In this case MSR measures our regression model's variability (separate from the variability of our error(MSE)).

MSE is the mean squared error and measures how close our regression line is to the data points. It measures the distances from the data points to our regression line. Those distances are the errors and the MSE is squaring those errors. __Ultimately__, it measures the variability/spread of those errors.


Both the MSR and MSE estimate the same quantity when the slope  $\beta_{1}$ is zero or not.


## __(c). At $\alpha$ = 0.05, conduct an F-test of whether or not $\beta_{1}$ = 0. State the null and alternative hypotheses, decision rule, and conclusion.__

<div align="center">

Null hypothesis: H0: $\beta_{1}$ = 0 (slope is horizontal/ no relationship)

Alternative hypothesis: H1: $\beta_{1}$ /= 0 (slope exists/ relationship exists)

</div>

Given our F-value is 9.2402

__note: null rejection rules__

_1. If the f statistic is larger than the critical value_

_2. Or the p value is less than alpha then it is significant and we reject the null_

```{r}
#Using qt() to find our critical value we get


qt(0.95, 118)

```

The F-value is greater than our critical value (1.980272) __AND__ our p-value (0.002917) is less than our $\alpha$ = 0.05 we reject our null and therefore a non-zero slope exists, as well as a significant relationship between ACT(X) and GPA(Y) scores.


## __(d).  Obtain the R-squared from your regression. Interpret this number__


<div align="center">

- R squared = 1 - unexplained variation/total variation

- R squared = variability in Y explained by X/total variability

- R squared = SSR(sum of squares of regression)/SST(sum of squares of total variation(SSR+SSE))

- R squared = 1 - SSE(sum of squares of error)/SST(sum of squares of total variation(SSR+SSE))


</div>

```{r}
#Using R functions to find R squared
summary(gpa_lm)$r.squared

#Reading from ANOVA table we can calculate manually
anova(gpa_lm)

#SSR(sum of squares of regression)
SSR_gpa <- 3.58

#SSE(sum of squares of errors)
SSE_gpa <- 45.818

#SST(sum of squares of total variation)

SST_gpa <- SSR_gpa + SSE_gpa

#R squared
rsquared_gpa <- SSR_gpa/ SST_gpa
rsquared_gpa
```


The R squared we found [1] 0.07247257 indicates that our ACT scores (input variable) explains close 7% of the variability in our dependent variable GPA. __The relationship between our model and the dependent variable GPA is very weak.__


--------------------------------------------

# __2. Refer to the Muscle Mass problem in HW#1.__

### __Workspace Setup__

```{r}
muscle_data = read.table(file = "Muscle.txt", header = FALSE, sep = "")

#Adding headers
names(muscle_data) <- c("Muscle", "Age")

#Defining dependent and independent vars
Age = muscle_data$Age #X
Muscle = muscle_data$Muscle #Y

#creating our linear model
muscle_lm = lm(Muscle ~ Age, data = muscle_data)
summary(muscle_lm)
```


## __(a). Setting up ANOVA Table__

```{r}
anova(muscle_lm)
```

## __(b). At $\alpha$ = 0.05, conduct an F-test of whether or not $\beta_{1}$ = 0. State the null and alternative hypotheses, decision rule, and conclusion.__


<div align="center">

Null hypothesis: H0: $\beta_{1}$ = 0 (slope is horizontal/ no relationship between X and Y)

Alternative hypothesis: H1: $\beta_{1}$ /= 0 (slope is non zero and X is linearly related to Y)

</div>

```{r}
#Using qt() to find our critical value we get
#Since alternative is not equal we're looking at both tails
# .025 on both sides
qt(0.975, 58)
```
Our F-statistic = 174.06

The F-value (174.06) is greater than our critical value (2.001717) __AND__ our p-value (2.2e-16) is less than our $\alpha$ = 0.05, we reject our null and therefore a non-zero slope exists, as well as a significant relationship between AGE(X) and MUSCLE(Y).

## __(c). What proportion of the total variation in Muscle Mass remains “unexplained” in the regression with Age? Is this proportion relatively small or large?__

The unexplained variation is the error component of the regression equation. It is the mean squared error (MSE) which is the sum of squares divided by the degrees of freedom.

```{r}
#explained variation, From summary
rsquared.muscle <- 0.7501
#Unexplained variation
1 - rsquared.muscle
```
~25% is unexplained, this number is small and therefore our model is strong since our explained variability is ~75%.

The ratio  11627.5 (MSR)/ 66.8 (MSE) shows a ratio that is fairly relatively large. We can definitely see that our model is contributing far more to the variance than the error is. Which is also true when we compare our F statistic (174.06) with the critical value (2.001717) and see that our model is a significant in predicting the variance between age and muscle mass.


## __(d).  Obtain the R-squared from your regression. Interpret this number__


```{r}
#Using R functions to find R squared
summary(muscle_lm)$r.squared

#Reading from ANOVA table we can calculate manually
anova(muscle_lm)

#SSR(sum of squares of regression)
SSR_muscle <- 11627.5

#SSE(sum of squares of errors)
SSE_muscle <- 3874.4

#SST(sum of squares of total variation)

SST_muscle <- SSR_muscle + SSE_muscle

#R squared
rsquared_muscle <- SSR_muscle / SST_muscle
rsquared_muscle
```


The R squared we found [1] 0.7500693 indicates that Age (input variable) explains helps explain close to 75% of the variability in our dependent variable Muscle. In other words, __the relationship between our model and the dependent variable Muscle is strong.__

# __3. Refer to the GPA problem in HW #1__


## __(a). Compute the Pearson correlation coefficient and attach the appropriate sign.__


```{r}
#Computing the sample correlation of two variables
#Choose any continuous variable, order does not matter
cor.test(ACT, GPA, method = "pearson")
```
The result of our Pearson correlation coefficient is a positive 0.269. 

__$r = +0.27$__

## __(b). Obtain Spearman rank correlation coefficient.__

```{r, warning = FALSE, message = FALSE}
#spearman correlation
cor.test(ACT, GPA, method = "spearman")
```
The result of our Spearman correlation coefficient is 0.31.


__$r_{s}=0.31$__


## __(c). Which correlation coefficient is stronger? Why?__

The Spearman correlation coefficient is stronger compared to the Pearson correlation between the variables ACT(X) and GPA(Y). This is because Spearman measures the data __points by ranks__. In other words, ACT and GPA are better suited because they are both __ordinal__ types of data indicating a kind of ranking system, whereas Pearson does not.


----------------------------------------------

# __4. Refer to the Muscle Mass problem in HW#1.__

## __(a). Compute the Pearson product-moment correlation coefficient.__

```{r}
#Age is x, Muscle is Y. But still placement order does not matter
cor.test(Age, Muscle, method = "pearson")
```
The result of our Pearson correlation coefficient is a negative 0.86 indicating a fairly strong negative linear relationship between our variables, age and muscle. 

__$r = -0.86$__


## __(b). Based on Part (a), test whether muscle mass and age are significantly correlated in the population at alpha = 0.05. State the null and alternative hypotheses, decision rule, and conclusion.__

- 1. __Pearson Correlation Hypothesis Testing Stating Our Hypotheses:__ 
  + Null hypothesis: $H_{0}$: $\rho = 0$ (There is no statistically significant linear correlation between Age and Muscle)
  + Alternative hypothesis: $H_{1}$: $\rho \neq 0$ (There exists a statistically significant linear correlation between Age and Muscle)


 
```{r}
summary(muscle_lm)

#Helps us find our T value
cor.test(Age, Muscle, method = "pearson")

#Gives us our critical value so we can compare with T statistic
qt(0.975, 58)
```
- 2. Calculating t statistic:
  + $t= -13.193$ 
  + Critical value = 2.00

- 3. Comparing with critical value and p value
  + |$t= -13.193$| > 2.00
  + p value  2.2e-16 < $\alpha$ (0.05)
- 4. Draw conclusion
  + Also, since the p value is very small and we can reject the NULL hypothesis that there is no linear correlation between age and muscle.
  + Therefore we can conclude with the alternative hypothesis that age and muscle are linearly correlated.

## __(c). Compute the Spearman rank correlation coefficient.__
```{r}
#Age is x, Muscle is Y. But still placement order does not matter
cor.test(Age, Muscle, method = "spearman")
```
## __(d). Repeat the test in Part (b) using the Spearman rank correlation from Part (c).__

- 1. __Spearman Correlation Hypothesis Testing Stating our Hypotheses:__ 
  + Null hypothesis: $H_{0}$: $\rho = 0$ (There is no statistically significant rank correlation between Age and Muscle)
  + Alternative hypothesis: $H_{1}$: $\rho \neq 0$ (There exists a statistically significant rank correlation between Age and Muscle)

```{r, warning = FALSE, message = FALSE}
#Helps us find our T value
cor.test(Age, Muscle, method = "spearman")

#Gives us our critical value so we can compare with T statistic
qt(0.975, 58)
```
 
- 2. Calculating t statistic:
  + $t= -13.193$ 
  + Critical value = 2.00

- 3. Comparing with critical value and p value
  + |$t= -13.193$| > 2.00
  + p value  2.2e-16 < $\alpha$ (0.05)
- 4. Draw conclusion
  + Similarly, since the p value is very small we can reject the NULL hypothesis that there is no significant rank correlation between age and muscle.
  + Therefore we can conclude with the alternative hypothesis that there is a significant rank correlation between age and muscle.


## __(e). How do your correlation coefficient estimates and test conclusions in Parts (a) and (b) compare to those obtained in Parts (c) and (d)?__

Based on the results from our hypothesis tests on both Pearson and Spearman, both age and muscle hold a significant rank and linear correlation relationship. These two hypothesis help verify our correlation coefficient estimates and allow us to safely conclude that a correlation does exist.











